import pandas as pd
import torch
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizerFast  # Use the fast tokenizer
from transformers import BertForSequenceClassification, Trainer, TrainingArguments

from config.logger import logger
from src.model.cve_repository import CVERepository


class CVEDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, item):
        encoding = {key: tensor[item] for key, tensor in self.encodings.items()}
        encoding["labels"] = torch.tensor(self.labels[item], dtype=torch.long)
        return encoding


class PredictMalwareTypeWithNLP:
    def __init__(self, input_repository: CVERepository, output_repository: CVERepository):
        self._input_repository = input_repository
        self._output_repository = output_repository

    def __call__(self):
        cves = self._input_repository.find_all()
        df = pd.DataFrame(cves, columns=["description", "malware_type"])
        df["full_text"] = df["description"]

        cves_with_malware_type_df = df[
            df["malware_type"].notnull()
            & (df["malware_type"] != "")
            & (df["malware_type"] != "unknown")
        ]

        # cves_with_malware_type_df = cves_with_malware_type_df[:100]

        # Encode the labels
        label_encoder = LabelEncoder()
        cves_with_malware_type_df["label"] = label_encoder.fit_transform(
            cves_with_malware_type_df["malware_type"]
        )

        # Split data into training and testing sets
        train_df, test_df = train_test_split(
            cves_with_malware_type_df, test_size=0.2, random_state=42
        )

        # Load the BERT tokenizer (Fast tokenizer)
        tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

        # Batch tokenization
        train_encodings = tokenizer(
            train_df["full_text"].tolist(),
            add_special_tokens=True,
            max_length=128,
            padding=True,
            truncation=True,
            return_attention_mask=True,
            return_tensors="pt",
        )

        test_encodings = tokenizer(
            test_df["full_text"].tolist(),
            add_special_tokens=True,
            max_length=128,
            padding=True,
            truncation=True,
            return_attention_mask=True,
            return_tensors="pt",
        )

        train_dataset = CVEDataset(train_encodings, train_df["label"].to_numpy())
        test_dataset = CVEDataset(test_encodings, test_df["label"].to_numpy())

        model = BertForSequenceClassification.from_pretrained(
            "bert-base-uncased", num_labels=len(cves_with_malware_type_df["label"].unique())
        )

        # Define training arguments with mixed precision
        training_args = TrainingArguments(
            output_dir="./data/output/nlp/results",
            num_train_epochs=3,
            per_device_train_batch_size=8,  # Increase batch size
            per_device_eval_batch_size=8,
            warmup_steps=10,
            weight_decay=0.01,
            logging_dir="./logs",
            logging_steps=10,
            evaluation_strategy="epoch",
            fp16=True,  # Enable mixed-precision training
            dataloader_num_workers=4,  # Utilize multiple workers for data loading
            gradient_accumulation_steps=4,  # Simulate larger batch size
        )

        # Create a Trainer instance
        trainer = Trainer(
            model=model, args=training_args, train_dataset=train_dataset, eval_dataset=test_dataset
        )

        # Train the model
        trainer.train()

        # Evaluate the model
        trainer.evaluate()

        # Predict on new data
        predictions = trainer.predict(test_dataset)
        predicted_labels = predictions.predictions.argmax(axis=1)
        predicted_malware_types = label_encoder.inverse_transform(predicted_labels)

        logger.info(predicted_malware_types)

        # Save predictions to a CSV file
        output_df = test_df.copy()  # Copy the test_df to add predictions
        output_df["predicted_malware_type"] = predicted_malware_types
        output_df.to_csv("predicted_malware_types.csv", index=False)

        logger.info("Predictions saved to predicted_malware_types.csv")
