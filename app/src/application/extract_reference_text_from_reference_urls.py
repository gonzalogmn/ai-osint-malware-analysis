import re

import nltk
import pandas as pd
import requests
from bs4 import BeautifulSoup
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

from config.logger import logger
from src.model.cve_repository import CVERepository


class ExtractReferenceTextFromReferenceURLs:
    def __init__(self, input_repository: CVERepository, output_repository: CVERepository):
        nltk.download("punkt")
        nltk.download("stopwords")
        self._input_repository = input_repository
        self._output_repository = output_repository

    def __get_reference_text(self, row):
        reference_text = ""
        if row["references"] is None or row["references"] == "" or pd.isna(row["references"]):
            return reference_text
        references = row["references"].split(",")
        for reference_url in references:
            try:
                response = requests.get(reference_url, timeout=30)
                response.raise_for_status()
            except requests.Timeout:
                logger.error(f"url: {reference_url}. The request timed out.")
                response = None
            except requests.RequestException as e:
                logger.error(f"url: {reference_url}. An error occurred: {e}")
                response = None

            if response:
                soup = BeautifulSoup(response.text, "html.parser")

                # Extract the news article content
                paragraphs = soup.find_all("p")
                reference_text = reference_text + (" ".join([para.text for para in paragraphs]))

                # Remove HTML tags
                text = re.sub(r"<.*?>", "", reference_text)

                # Remove punctuation and lowercase the text
                text = re.sub(r"[^\w\s]", "", text).lower()

                # Tokenize the text
                tokens = word_tokenize(text)

                # Remove stopwords
                stop_words = set(stopwords.words("english"))
                tokens = [word for word in tokens if word not in stop_words]

                reference_text = reference_text + (" ".join(tokens))

                reference_text = (
                    reference_text.replace("\t", " ")
                    .replace("\n", " ")
                    .replace("\r", " ")
                    .replace(",", " ")
                    .replace(";", " ")
                    .replace('"', " ")
                )

            logger.info(f"{row['cve_id']}: {reference_text}")
            return reference_text

    def __call__(self):
        logger.info("Starting ExtractReferenceTextFromReferenceURLs use case...")
        documents = self._input_repository.find_all()
        df = pd.DataFrame(documents)

        for index, row in df.iterrows():
            output_row = self._output_repository.find_one(row["cve_id"])
            if not output_row or ("reference_text" not in output_row):
                row["reference_text"] = self.__get_reference_text(row)
                cve = row.to_dict()
                self._output_repository.update_reference_text(cve)
                logger.info(f"{row['cve_id']} saved with 'reference_text'")
            else:
                logger.info(f"{row['cve_id']} has already a 'reference_text'")
