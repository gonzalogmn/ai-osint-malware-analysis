import os

import pandas as pd
import torch
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from torch.utils.data import DataLoader, Dataset
from transformers import (
    BertForSequenceClassification,
    BertTokenizerFast,
    Trainer,
    TrainingArguments,
)

from config.logger import logger
from src.model.cve_repository import CVERepository

CHUNK_SIZE = 1000


class CVEDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, item):
        encoding = {key: tensor[item] for key, tensor in self.encodings.items()}
        encoding["labels"] = torch.tensor(self.labels[item], dtype=torch.long)
        return encoding


class PredictExploitTagWithNLP:
    def __init__(self, input_repository: CVERepository, output_repository: CVERepository):
        self._input_repository = input_repository
        self._output_repository = output_repository
        self._model_dir = "./data/models/exploit_type/nlp/model"

    def __call__(self):
        logger.info("Starting PredictExploitTagWithNLP use case...")
        cves = self._input_repository.find_all()
        original_df = pd.DataFrame(cves)
        df = pd.DataFrame(
            cves, columns=["description", "exploit_description", "exploit_tag_generated"]
        )
        df["full_text"] = df["description"].fillna("") + df["exploit_description"].fillna("")

        cves_with_exploit_tag_df = df[
            df["exploit_tag_generated"].notnull()
            & (df["exploit_tag_generated"] != "")
            & (df["exploit_tag_generated"] != "unknown")
        ]

        cves_without_exploit_tag_df = df[
            (df["exploit_tag_generated"] == "")
            | (df["exploit_tag_generated"] is None)
            | (df["exploit_tag_generated"].isna())
            | (df["exploit_tag_generated"] == "unknown")
        ]

        # TODO only for testing
        # cves_with_exploit_tag_df = cves_with_exploit_tag_df[:100]
        # cves_without_exploit_tag_df = cves_without_exploit_tag_df[:100]

        logger.info(
            f"original_df: {len(original_df)} df: {len(df)} cves_with_exploit_tag_df: {len(cves_with_exploit_tag_df)} cves_without_exploit_tag_df: {len(cves_without_exploit_tag_df)} "
        )
        # Encode the labels
        label_encoder = LabelEncoder()
        cves_with_exploit_tag_df["label"] = label_encoder.fit_transform(
            cves_with_exploit_tag_df["exploit_tag_generated"]
        )

        # Split data into training and testing sets
        train_df, test_df = train_test_split(
            cves_with_exploit_tag_df, test_size=0.2, random_state=42
        )

        # Load the BERT tokenizer (Fast tokenizer)
        tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

        # Batch tokenization
        train_encodings = tokenizer(
            train_df["full_text"].tolist(),
            add_special_tokens=True,
            max_length=128,
            padding=True,
            truncation=True,
            return_attention_mask=True,
            return_tensors="pt",
        )

        test_encodings = tokenizer(
            test_df["full_text"].tolist(),
            add_special_tokens=True,
            max_length=128,
            padding=True,
            truncation=True,
            return_attention_mask=True,
            return_tensors="pt",
        )

        train_dataset = CVEDataset(train_encodings, train_df["label"].to_numpy())
        test_dataset = CVEDataset(test_encodings, test_df["label"].to_numpy())

        logger.info("Defining training arguments...")
        # Define training arguments with mixed precision
        training_args = TrainingArguments(
            output_dir=self._model_dir,
            num_train_epochs=3,
            per_device_train_batch_size=8,  # Increase batch size
            per_device_eval_batch_size=8,
            warmup_steps=10,
            weight_decay=0.01,
            logging_dir="./logs",
            logging_steps=10,
            evaluation_strategy="epoch",
            fp16=True,  # Enable mixed-precision training
            dataloader_num_workers=4,  # Utilize multiple workers for data loading
            gradient_accumulation_steps=4,  # Simulate larger batch size
        )

        if not os.path.exists(self._model_dir):

            model = BertForSequenceClassification.from_pretrained(
                "bert-base-uncased", num_labels=len(cves_with_exploit_tag_df["label"].unique())
            )

            logger.info("Creating Trainer instance...")
            # Create a Trainer instance
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=test_dataset,
            )

            logger.info("Training model...")
            trainer.train()
            logger.info("Finished training")

            logger.info("Saving model and tokenizer...")
            model.save_pretrained(self._model_dir)
            tokenizer.save_pretrained(self._model_dir)
            logger.info("Finished model and tokenizer saving")

        else:
            logger.info("Loading the model from disk...")
            model = BertForSequenceClassification.from_pretrained(self._model_dir)
            tokenizer = BertTokenizerFast.from_pretrained(self._model_dir)

        # Create a Trainer instance
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=test_dataset,
        )

        logger.info("Evaluating model...")
        trainer.evaluate()
        logger.info("Finished evaluation")

        total_size = len(cves_without_exploit_tag_df)
        for start in range(0, total_size, CHUNK_SIZE):
            end = start + CHUNK_SIZE
            logger.info(f"Predicting CVEs {start}-{end} of {total_size}")
            chunk_cves_without_exploit_tag_df = cves_without_exploit_tag_df.iloc[start:end]

            ft = chunk_cves_without_exploit_tag_df["full_text"].tolist()
            cves_without_exploit_tag_encodings = tokenizer(
                chunk_cves_without_exploit_tag_df["full_text"].tolist(),
                add_special_tokens=True,
                max_length=128,
                padding=True,
                truncation=True,
                return_attention_mask=True,
                return_tensors="pt",
            )

            cves_without_exploit_tag_dataset = CVEDataset(
                cves_without_exploit_tag_encodings,
                torch.zeros(len(chunk_cves_without_exploit_tag_df)).long(),
            )

            logger.info("Predicting on new data...")
            predictions = trainer.predict(cves_without_exploit_tag_dataset)
            predicted_labels = predictions.predictions.argmax(axis=1)

            # Apply softmax to get probabilities
            probabilities = F.softmax(torch.tensor(predictions.predictions), dim=-1)

            # Get the highest probability and its corresponding label
            max_probs, predicted_labels = torch.max(probabilities, dim=-1)

            logger.info("Finished predicting on new data")

            predicted_exploit_types = []
            prediction_confidence = []
            for prob, label in zip(max_probs, predicted_labels):
                prediction_confidence.append(prob.item())
                try:
                    predicted_exploit_types.append(label_encoder.inverse_transform([label])[0])
                except ValueError:
                    predicted_exploit_types.append("unknown")

            columns_to_include = list(cves_with_exploit_tag_df.columns) + [
                "predicted_exploit_tag",
                "predicted_exploit_tag_confidence",
            ]

            # Create the `output_df` with the predicted labels
            output_df = chunk_cves_without_exploit_tag_df.copy()
            output_df["predicted_exploit_tag"] = predicted_exploit_types
            output_df["predicted_exploit_tag_confidence"] = prediction_confidence

            # Adjust the `output_df` to include all necessary columns
            output_df = output_df.reindex(columns=columns_to_include, fill_value="")

            logger.info(output_df.head())

            merged_rows = []
            for index, row in output_df.iterrows():
                original_row = original_df[original_df["description"] == row["description"]]

                if not original_row.empty:
                    merged_row = {
                        "cve_id": original_row.iloc[0]["cve_id"],
                        "affected_product": original_row.iloc[0]["affected_product"],
                        "affected_vendor": original_row.iloc[0]["affected_vendor"],
                        "capec_id": original_row.iloc[0]["capec_id"],
                        "date_published": original_row.iloc[0]["date_published"],
                        "description": original_row.iloc[0]["description"],
                        "label": row["label"],
                        "exploit_tag_x": original_row.iloc[0]["exploit_tag_generated"],
                        "exploit_tag_y": row["exploit_tag_generated"],
                        "predicted_exploit_tag": row["predicted_exploit_tag"],
                        "predicted_exploit_tag_confidence": row[
                            "predicted_exploit_tag_confidence"
                        ],
                        "references": original_row.iloc[0]["references"],
                    }

                    merged_rows.append(merged_row)

            merged_df = pd.DataFrame(merged_rows)
            logger.info(
                f"output_df size: {len(output_df)}, original_df size: {len(original_df)}, merged_df size: {len(merged_df)}"
            )

            data_dict = merged_df.to_dict("records")
            for cve in data_dict:
                self._output_repository.save(cve)

        merged_rows = []
        for index, row in cves_with_exploit_tag_df.iterrows():
            original_row = original_df[original_df["description"] == row["description"]]

            if not original_row.empty:
                merged_row = {
                    "cve_id": original_row.iloc[0]["cve_id"],
                    "affected_product": original_row.iloc[0]["affected_product"],
                    "affected_vendor": original_row.iloc[0]["affected_vendor"],
                    "capec_id": original_row.iloc[0]["capec_id"],
                    "date_published": original_row.iloc[0]["date_published"],
                    "description": original_row.iloc[0]["description"],
                    "exploit_tag_x": original_row.iloc[0]["exploit_tag_generated"],
                    "exploit_tag_y": row["exploit_tag_generated"],
                    "predicted_exploit_tag": row["exploit_tag_generated"],
                    "predicted_exploit_tag_confidence": 100.0,
                    "references": original_row.iloc[0]["references"],
                }

                merged_rows.append(merged_row)

        merged_df = pd.DataFrame(merged_rows)
        logger.info(
            f"output_df size: {len(cves_with_exploit_tag_df)}, original_df size: {len(original_df)}, merged_df size: {len(merged_df)}"
        )

        data_dict = merged_df.to_dict("records")
        for cve in data_dict:
            self._output_repository.save(cve)
